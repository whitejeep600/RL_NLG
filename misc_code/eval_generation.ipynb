# I ran the code below on Google Colab to obtain evaluations for the
# different decoding strategies. I provide the run_summarization_no_trainer.py
# file in this directory. It is a slightly modified version of the original
# program from the Huggingface library.

# If run, the program looks like it begins the dataset tokenization and then stops.
# This is not the case. I modified the original script in a bit of a chaotic way, 
# so for example the evaluation progress bar is not shown. A typical evaluation
# time is 30 to 40 minutes.

from google.colab import drive
drive.mount('/content/drive')
!ls '/content/drive/MyDrive'
%cd '/content/drive/My Drive'

!pip install sentencepiece
!pip install git+https://github.com/huggingface/transformers
!pip install datasets
!pip install evaluate
!pip install accelerate
!pip install rouge_score

!python /content/drive/MyDrive/run_summarization_no_trainer.py \
    --model_name_or_path '/content/drive/My Drive/summarization/model_fp16' \
    --validation_file='/content/drive/My Drive/summarization/data/public.csv'\ # preformatted
    --source_prefix "summarize: " \
    --output_dir '/content/drive/My Drive/summarization/data/out/new_summary_p085' \
    --decoder='top_p' \ # for example
    --p=0.85 \
    --val_max_target_length=256 \
    --per_device_eval_batch_size=1 \
    --num_train_epochs=1
